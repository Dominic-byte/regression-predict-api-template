{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dominic-byte/regression-predict-api-template/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2XYUCCv2rGM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    Helper functions for the pretrained model to be used within our API.\\n\\n    Author: Explore Data Science Academy.\\n\\n    Note:\\n    ---------------------------------------------------------------------\\n    Plase follow the instructions provided within the README.md file\\n    located within this directory for guidance on how to use this script\\n    correctly.\\n\\n    Importantly, you will need to modify this file by adding\\n    your own data preprocessing steps within the `_preprocess_data()`\\n    function.\\n    ----------------------------------------------------------------------\\n\\n    Description: This file contains several functions used to abstract aspects\\n    of model interaction within the API. This includes loading a model from\\n    file, data preprocessing, and model prediction.  \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "    Helper functions for the pretrained model to be used within our API.\n",
    "\n",
    "    Author: Explore Data Science Academy.\n",
    "\n",
    "    Note:\n",
    "    ---------------------------------------------------------------------\n",
    "    Plase follow the instructions provided within the README.md file\n",
    "    located within this directory for guidance on how to use this script\n",
    "    correctly.\n",
    "\n",
    "    Importantly, you will need to modify this file by adding\n",
    "    your own data preprocessing steps within the `_preprocess_data()`\n",
    "    function.\n",
    "    ----------------------------------------------------------------------\n",
    "\n",
    "    Description: This file contains several functions used to abstract aspects\n",
    "    of model interaction within the API. This includes loading a model from\n",
    "    file, data preprocessing, and model prediction.  \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjdaShSi2rGQ"
   },
   "outputs": [],
   "source": [
    "# Helper Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ob6aQol72rGS"
   },
   "outputs": [],
   "source": [
    "def _preprocess_data(data):\n",
    "    \"\"\"Private helper function to preprocess data for model prediction.\n",
    "\n",
    "    NB: If you have utilised feature engineering/selection in order to create\n",
    "    your final model you will need to define the code here.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str\n",
    "        The data payload received within POST requests sent to our API.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame : <class 'pandas.core.frame.DataFrame'>\n",
    "        The preprocessed data, ready to be used our model for prediction.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert the json string to a python dictionary object\n",
    "    feature_vector_dict = json.loads(data)\n",
    "    # Load the dictionary as a Pandas DataFrame.\n",
    "    feature_vector_df = pd.DataFrame.from_dict([feature_vector_dict])\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # NOTE: You will need to swap the lines below for your own data\n",
    "    # preprocessing methods.\n",
    "    #\n",
    "    # The code below is for demonstration purposes only. You will not\n",
    "    # receive marks for submitting this code in an unchanged state.\n",
    "    # ---------------------------------------------------------------\n",
    "    def col_rename(df):\n",
    "        \"\"\"Function that renames columns and drops columns that \n",
    "        were not used in the test set\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: DataFrame\n",
    "              input df to rename columns for\n",
    "        Returns\n",
    "        -------\n",
    "        new_df: DataFrame\n",
    "              output df with renamed and dropped columns\n",
    "              as well as a printout of dropped columns\n",
    "        \"\"\"\n",
    "        static_dict = {\"Order No\":\"Order No\",\"User Id\":\"User Id\",\"Vehicle Type\":\"Vehicle Type\",\"Platform Type\":\"Platform\",\n",
    "            \"Personal or Business\":\"Pers Business\",\"Placement - Day of Month\":\"Place DoM\",\n",
    "            \"Placement - Weekday (Mo = 1)\":\"Place Weekday\",\"Placement - Time\":\"Place Time\",\n",
    "            \"Confirmation - Day of Month\":\"Confirm DoM\",\"Confirmation - Weekday (Mo = 1)\":\"Confirm Weekday\",\n",
    "            \"Confirmation - Time\":\"Confirm Time\",\"Arrival at Pickup - Day of Month\":\"Arr Pickup DoM\",\n",
    "            \"Arrival at Pickup - Weekday (Mo = 1)\":\"Arr Pickup Weekday\",\"Arrival at Pickup - Time\":\"Arr Pickup Time\",\n",
    "            \"Pickup - Day of Month\":\"Pickup DoM\",\"Pickup - Weekday (Mo = 1)\":\"Pickup Weekday\",\"Pickup - Time\":\"Pickup Time\",\n",
    "            \"Distance (KM)\":\"Distance KM\",\"Temperature\":\"Temperature\",\"Precipitation in millimeters\":\"Precipitation mm\",\n",
    "            \"Pickup Lat\":\"Pickup Lat\",\"Pickup Long\":\"Pickup Long\",\"Destination Lat\":\"Destination Lat\",\"Time from Pickup to Arrival\":\"Time from Pickup to Arrival\",\n",
    "            \"Destination Long\":\"Destination Long\",\"Rider Id\":\"Rider Id\"}\n",
    "        new_df = df.copy()\n",
    "        new_cols = {}\n",
    "        droplist = []\n",
    "        for col in new_df.columns:\n",
    "            if col in static_dict.keys():\n",
    "                new_cols[col] = static_dict[col].replace(\" \",\"_\")\n",
    "        else:\n",
    "            droplist.append(col)\n",
    "        new_df.rename(columns = new_cols, inplace=True)\n",
    "        for col in droplist:\n",
    "            if col in new_df.index:\n",
    "                return new_df.drop(columns=droplist, inplace=True)\n",
    "            return new_df\n",
    "    \n",
    "    feature_vector_df = col_rename(feature_vector_df)\n",
    "    \n",
    "    def data_preprocessing(df):\n",
    "        \"\"\"Function that preprocesses data used for predictions and testing\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: DataFrame\n",
    "              DataFrame to preprocess.\n",
    "        Returns\n",
    "        -------\n",
    "        df: DataFrame\n",
    "              Preprocessed DataFrame\n",
    "\n",
    "        \"\"\"\n",
    "        cdf = df.copy()\n",
    "        old = cdf.memory_usage().sum()\n",
    "        # Create lists of columns to change\n",
    "        integers = [col for col in cdf.columns if cdf[col].dtypes == 'int64']\n",
    "        floats = [col for col in cdf.columns if cdf[col].dtypes == 'float64']\n",
    "        time_cols = [col for col in cdf.columns if col.endswith(\"Time\")]\n",
    "\n",
    "        # Reduce size of data storage types\n",
    "        cdf[integers] = cdf[integers].astype('int16')\n",
    "        cdf[floats] = cdf[floats].astype('float16')\n",
    "        for col in time_cols:\n",
    "            cdf[col] = pd.to_datetime(cdf[col])\n",
    "            cdf[col] = [time.time() for time in cdf[col]]\n",
    "            cdf[col] = cdf[col].apply(lambda x: x.hour)\n",
    "        # Correcting specific columns\n",
    "        if 'Distance (KM)' in cdf.columns:\n",
    "            cdf['Distance (KM)'] = cdf['Distance (KM)'].astype('float16') \n",
    "\n",
    "        new = cdf.memory_usage().sum()\n",
    "        print(\"Bytes\\t\",\"\\nOld:\\t\", str(old), \"\\nNew:\\t\", \"(\"+str(new)+\")\", \"\\nSaved:\\t\", str(old - new))\n",
    "        return cdf\n",
    "    \n",
    "    feature_vector_df = data_preprocessing(feature_vector_df)\n",
    "    \n",
    "    def collinear(df):\n",
    "        \"\"\"Function that drops chosen columns\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: DataFrame\n",
    "              DataFrame to drop columns from\n",
    "        cols: list-like\n",
    "              Names of columns to be dropped from\n",
    "        Returns\n",
    "        -------\n",
    "        col_df: DataFrame\n",
    "              Modified DataFrame\n",
    "        \"\"\"\n",
    "    #     col_df = pd.merge(df, eda_riders, how='left', on='Rider_Id')\n",
    "        droplist = ['Place_DoM', 'Place_Weekday',\n",
    "                  'Place_Time', 'Confirm_DoM', \n",
    "                  'Confirm_Weekday', 'Confirm_Time',\n",
    "                  'Arr_Pickup_DoM', 'Arr_Pickup_Weekday',\n",
    "                  'Arr_Pickup_Time']\n",
    "        if droplist[0] in col_df.columns:\n",
    "            col_df = col_df.drop(columns=droplist)\n",
    "\n",
    "        return col_df\n",
    "\n",
    "    feature_vector_df = collinear(feature_vector_df)\n",
    "    \n",
    "    def null(df, na_thresh = 1.0 , strategy = \"median\"):\n",
    "        \"\"\"Function that drops columns with more than na_thresh null from\n",
    "\n",
    "        \"\"\"\n",
    "        no_null=df.copy()\n",
    "        # Dropping NaN's\n",
    "        for col in no_null.columns:\n",
    "            missing = no_null[col].isnull().sum()/len(no_null)\n",
    "        if missing > na_thresh:\n",
    "            no_null.drop(columns=col, inplace=True)\n",
    "\n",
    "        # Filling NaN's\n",
    "        for col in no_null.columns:\n",
    "            if no_null[col].isnull().sum() > 0:\n",
    "                if no_null[col].dtypes == 'object':\n",
    "                    no_null[col].fillna(no_null[col].mode(), inplace=True) \n",
    "                elif strategy == 'mean':\n",
    "                    no_null[col].fillna(round(no_null[col].mean()), inplace=True)\n",
    "                elif strategy == 'median':\n",
    "                    no_null[col].fillna(round(no_null[col].median()), inplace=True)\n",
    "                elif strategy == 'rolling':\n",
    "                    no_null[col].fillna(no_null[col].rolling(7).mean(), inplace=True)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "        return no_null    \n",
    "    \n",
    "    def variable_transformer(df):\n",
    "        \"\"\"Function to apply variable ALL transformations to dataset\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: DataFrame\n",
    "              Input df to be transformed\n",
    "        Returns\n",
    "        -------\n",
    "        trans_df: DataFrame\n",
    "              Output df\n",
    "        \"\"\"\n",
    "        index = df.index\n",
    "        trans_df = df.copy()\n",
    "\n",
    "        # # Time to number\n",
    "        # for time in time_cols:\n",
    "        #   if time in trans_df.columns:\n",
    "        #     trans_df[time] = pd.to_datetime(trans_df[time])\n",
    "\n",
    "\n",
    "\n",
    "        # # Feature scaling\n",
    "        # predictor_cols = ['Distance_KM'\t,'Temperature\t','Precipitation_mm', 'No_Of_Orders', 'Age', 'Average_Rating',\t'No_of_Ratings']\n",
    "        # sc = StandardScaler()\n",
    "        # sc.fit_transform(trans_df[predictor_cols])\n",
    "\n",
    "        # Drop unnecessary features\n",
    "        droplist = ['User_Id', 'Vehicle_Type', 'Rider_Id'] #, 'Pickup_Lat',\t'Pickup_Long',\t'Destination_Lat',\t'Destination_Long', 'Temperature','Precipitation_mm', 'Pay_Day', 'Pickup_DoM']\n",
    "        for col in droplist:\n",
    "            if col in trans_df.columns:\n",
    "                trans_df.drop(columns=col, inplace=True)\n",
    "\n",
    "        # Reorder columns\n",
    "        reindex_cols = [col for col in trans_df.columns if col != target] + [target]\n",
    "        trans_df = trans_df.reindex(columns = reindex_cols)\n",
    "        trans_df.index = (index)\n",
    "        original_features = [col for col in trans_df.columns if col != target]\n",
    "        return trans_df, original_features    \n",
    "\n",
    "    feature_vector_df, ori_feats = variable_transformer(feature_vector_df)\n",
    "    \n",
    "    def variable_creator(df):\n",
    "        \"\"\"Doctstring here\n",
    "        \"\"\"\n",
    "        create_df = df.copy()\n",
    "\n",
    "        # covert GPS corrdinates to distance\n",
    "        def dist(df,lat1,long1,lat2,long2):\n",
    "            list1=[]\n",
    "            geo_df = df.copy()\n",
    "            for i in range (0,len(geo_df)):\n",
    "                coords_1 = (geo_df[lat1][i], geo_df[long1][i])\n",
    "                coords_2 = (geo_df[lat2][i], geo_df[long2][i])\n",
    "\n",
    "                distance=geopy.distance.vincenty(coords_1, coords_2).m\n",
    "                #print(distance)\n",
    "                list1.append(distance)\n",
    "            geo_df['Geo_Distance'] = list1\n",
    "            return geo_df\n",
    "        create_df = dist(create_df,'Pickup_Lat', 'Pickup_Long', 'Destination_Lat',\n",
    "            'Destination_Long')\n",
    "\n",
    "        # # Extract hours from columns\n",
    "        # create_df['Pickup_Hour'] = pd.to_datetime(create_df['Pickup_Time'].astype(str)).dt.hour\n",
    "\n",
    "\n",
    "\n",
    "        # Determine which platform is busiest\n",
    "        def is_busy(platform):\n",
    "            if platform == 3:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        create_df['Platform_load'] = create_df['Platform'].apply(is_busy)\n",
    "\n",
    "        # Which drivers are the most experienced\n",
    "        def experience(age):\n",
    "            if age < 1000:\n",
    "                return 1\n",
    "            elif age < 2000:\n",
    "                return 2\n",
    "            elif age < 3000:\n",
    "                return 3\n",
    "            return 4\n",
    "        create_df['Driver_exp'] = create_df['Age'].apply(experience)\n",
    "\n",
    "        # Business day\n",
    "        def weekday(day):\n",
    "            if day in [1,2,3,4,5]:\n",
    "                return 1\n",
    "            return 0\n",
    "        create_df['Business_day'] = create_df['Pickup_Weekday'].apply(weekday)\n",
    "\n",
    "        # Is it hot?\n",
    "        def comfort(temp):\n",
    "\n",
    "            if temp < 18:\n",
    "                return 0\n",
    "            elif temp < 28.75:\n",
    "                return 1\n",
    "            return 2\n",
    "        create_df['Comfort'] = create_df['Temperature'].apply(comfort)\n",
    "\n",
    "        # Is pay day?\n",
    "        def pay_day(DoM):\n",
    "            if DoM in [30,31]:\n",
    "                return True\n",
    "            return False\n",
    "        create_df['Pay_Day'] = create_df['Pickup_DoM'].apply(pay_day)\n",
    "\n",
    "        # Get dummies\n",
    "        dummy_cols = ['Pers_Business','Pickup_Weekday', 'Pickup_DoM','Platform','Pickup_Time']\n",
    "        create_df = pd.get_dummies(create_df,columns = dummy_cols, drop_first=True,)\n",
    "\n",
    "        # Reorder columns\n",
    "        reindex_cols = [col for col in create_df.columns if col != target] + [target]\n",
    "        create_df = create_df.reindex(columns = reindex_cols)\n",
    "\n",
    "        create_features = [col for col in create_df.columns if col not in df.columns]\n",
    "\n",
    "        return create_df, create_features\n",
    "    feature_vector_df, new_feats = variable_creator(feature_vector_df) \n",
    "        \n",
    "    predictor_feats = [# 'Feature',  # Individual P-Score\n",
    "                  # 'Platform',\n",
    "                  # 'Pers_Business',\n",
    "                  # 'Pickup_DoM',\n",
    "                  # 'Pickup_Weekday',\n",
    "                  # 'Pickup_Time',\n",
    "                  'Distance_KM', # P - 0.00\n",
    "                  # 'Temperature', # P - 0.305\n",
    "                  # 'Precipitation_mm', # P - 0.216\n",
    "                  # 'Pickup_Lat', # location does not consider obstacles\n",
    "                  # 'Pickup_Long',  # location does not consider obstacles\n",
    "                  # 'Destination_Lat',  # location does not consider obstacles\n",
    "                  # 'Destination_Long', # location does not consider obstacles \n",
    "                  # 'No_Of_Orders', # condition number is large, 5.09e+03.\n",
    "                  # 'Age', # condition number is large, 2.88e+03.\n",
    "                  'Average_Rating', # P - 0.000\n",
    "                  # 'No_of_Ratings', # condition number is large, 1.11e+03.\n",
    "                  # 'Geo_Distance',  # condition number is large, 1.6e+04.\n",
    "                  # 'Platform_load', # P - 0.229\n",
    "                  'Driver_exp', # P - 0.000\n",
    "                  # 'Business_day', # P - 0.000 # multicollinearity\n",
    "                  # 'Comfort', # P - 0.104\n",
    "                  # 'Pay_Day', # P - 0.306\n",
    "                  'Pers_Business_Personal', # P - 0.002\n",
    "                  'Pickup_Weekday_2', # P - 0.000\n",
    "                  # 'Pickup_Weekday_3', # P - 0.369\n",
    "                  # 'Pickup_Weekday_4', # P - 0.123\n",
    "                  'Pickup_Weekday_5', # P - 0.000 \n",
    "                  'Pickup_Weekday_6', # P - 0.000\n",
    "                  # 'Pickup_Weekday_7', # P - 0.292\n",
    "                  # 'Pickup_DoM_2', # P - 0.939\n",
    "                  # 'Pickup_DoM_3', # P - 0.947\n",
    "                  # 'Pickup_DoM_4', # P - 0.355\n",
    "                  # 'Pickup_DoM_5', # P - 0.577\n",
    "                  # 'Pickup_DoM_6', # P - 0.737\n",
    "                  # 'Pickup_DoM_7', # P - 0.304\n",
    "                  'Pickup_DoM_8', # P - 0.023\n",
    "                  'Pickup_DoM_9', # P - 0.021\n",
    "                  # 'Pickup_DoM_10', # P - 0.725\n",
    "                  'Pickup_DoM_11', # P - 0.140\n",
    "                  # 'Pickup_DoM_12', # P - 0.332\n",
    "                  # 'Pickup_DoM_13', # P - 0.550\n",
    "                  # 'Pickup_DoM_14', # P - 0.288\n",
    "                  # 'Pickup_DoM_15', # P - 0.735\n",
    "                  # 'Pickup_DoM_16', # P - 0.195\n",
    "                  'Pickup_DoM_17', # P - 0.098\n",
    "                  # 'Pickup_DoM_18', # P - 0.229\n",
    "                  # 'Pickup_DoM_19', # P - 0.425\n",
    "                  # 'Pickup_DoM_20', # P - 0.845\n",
    "                  # 'Pickup_DoM_21', # P - 0.553\n",
    "                  'Pickup_DoM_22', # P - 0.098\n",
    "                  # 'Pickup_DoM_23', # P - 0.500\n",
    "                  # 'Pickup_DoM_24', # P - 0.627\n",
    "                  # 'Pickup_DoM_25', # P - 0.352\n",
    "                  # 'Pickup_DoM_26', # P - 0.245\n",
    "                  'Pickup_DoM_27', # P - 0.001\n",
    "                  # 'Pickup_DoM_28', # P - 0.746\n",
    "                  'Pickup_DoM_29', # P - 0.096\n",
    "                  'Pickup_DoM_30', # P - 0.079 \n",
    "                  # 'Pickup_DoM_31', # P - 0.565\n",
    "                  # 'Platform_2', # P - 0.342\n",
    "                  # 'Platform_3', # P - 0.229\n",
    "                  # 'Platform_4', # P - 0.155\n",
    "                  # 'Pickup_Time_7',  # P - 0.947\n",
    "                  'Pickup_Time_8',  # P - 0.043\n",
    "                  'Pickup_Time_9',  # P - 0.000\n",
    "                  'Pickup_Time_10', # P - 0.000\n",
    "                  # 'Pickup_Time_11', # P - 0.380\n",
    "                  'Pickup_Time_12', # P - 0.086\n",
    "                  # 'Pickup_Time_13', # P - 0.879\n",
    "                  # 'Pickup_Time_14', # P - 0.625\n",
    "                  'Pickup_Time_15', # P - 0.000\n",
    "                  'Pickup_Time_16', # P - 0.086\n",
    "                  # 'Pickup_Time_17', # P -0.325\n",
    "                  # 'Pickup_Time_18', # P - 0.585\n",
    "                  # 'Pickup_Time_19', # P - 0.741\n",
    "                  # 'Pickup_Time_20', # P - 0.461\n",
    "                  # 'Pickup_Time_21', # P - 0.300\n",
    "                  # 'Pickup_Time_22', # P - 0.158\n",
    "                  # 'Pickup_Time_23' # P - 0.514]\n",
    "    ]\n",
    "    # ----------- Replace this code with your own preprocessing steps --------\n",
    "        \n",
    "    predict_vector = feature_vector_df[predictor_feats].copy()\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    return predict_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-278602e370cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "predict_vector = _preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEAxdXD-2rGU"
   },
   "outputs": [],
   "source": [
    "def load_model(path_to_model:str):\n",
    "    \"\"\"Adapter function to load our pretrained model into memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_model : str\n",
    "        The relative path to the model weights/schema to load.\n",
    "        Note that unless another file format is used, this needs to be a\n",
    "        .pkl file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    <class: sklearn.estimator>\n",
    "        The pretrained model loaded into memory.\n",
    "\n",
    "    \"\"\"\n",
    "    return pickle.load(open(path_to_model, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIFOQUDo2rGX"
   },
   "outputs": [],
   "source": [
    "def make_prediction(data, model):\n",
    "    \"\"\"Prepare request data for model prediciton.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str\n",
    "        The data payload received within POST requests sent to our API.\n",
    "    model : <class: sklearn.estimator>\n",
    "        An sklearn model object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A 1-D python list containing the model prediction.\n",
    "\n",
    "    \"\"\"\n",
    "    # Data preprocessing.\n",
    "    prep_data = _preprocess_data(data)\n",
    "    # Perform prediction with model and preprocessed data.\n",
    "    prediction = model.predict(prep_data)\n",
    "    # Format as list for output standerdisation.\n",
    "    return prediction[0].tolist()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
